{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0d65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7c4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.battleships_torch import BattleshipsBoardCollection, StepOut\n",
    "from src.policies import Policy_10x10, sample_action_get_log_prob\n",
    "from src.losses import VanillaPolictGradientLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbdf91",
   "metadata": {},
   "source": [
    "### Simple vanilla policy gradient RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f50eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps'\n",
    "NUM_PARALLEL_GAMES = 1000\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2\n",
    "REWARD_FINAL = 10\n",
    "REWARD_HIT = 1\n",
    "STEP_PENALTY = -0.05\n",
    "REWARD_DISCOUNT = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6988231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)\n",
    "policy = Policy_10x10().to(device)\n",
    "boards = BattleshipsBoardCollection(batch_size=NUM_PARALLEL_GAMES, device=DEVICE)\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=LEARNING_RATE)\n",
    "loss = VanillaPolictGradientLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24754b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch: 100] [loss: 0.06555197387933731] [avg moves: 93.52300262451172] [avg entropy 0.5614566802978516]: 100%|██████████| 100/100 [05:47<00:00,  3.48s/it]\n"
     ]
    }
   ],
   "source": [
    "epoch_progress = tqdm(range(NUM_EPOCHS))\n",
    "for epoch in epoch_progress:\n",
    "\n",
    "    boards.reset()\n",
    "\n",
    "    # arrays for tensors to track rewards and log_probs\n",
    "    mean_entropies = []\n",
    "    all_log_probs = []\n",
    "    all_rewards = []\n",
    "    all_active = []\n",
    "\n",
    "    # loop until all games are finished\n",
    "    not_finished = True\n",
    "    while not_finished:\n",
    "        not_finished = False\n",
    "\n",
    "        # get action logits according to policy\n",
    "        states = boards.state()\n",
    "        logits = policy(states)\n",
    "\n",
    "        # sample actions, get their log_probs, and entropies of all actions to track early collapse\n",
    "        masks = boards.mask()\n",
    "        actions, log_probs, entropies = sample_action_get_log_prob(logits=logits, mask=masks)\n",
    "        \n",
    "        # carry out a step\n",
    "        step_out: StepOut = boards.step(actions)\n",
    "        not_finished = not step_out.all_finished\n",
    "\n",
    "        # calculate rewards\n",
    "        active_mask = step_out.active\n",
    "        hit = step_out.hit\n",
    "        won = step_out.won\n",
    "\n",
    "        rewards = torch.full((NUM_PARALLEL_GAMES,), STEP_PENALTY, device=device, dtype=torch.float32)\n",
    "        rewards += hit.to(rewards.dtype) * REWARD_HIT\n",
    "        rewards += won.to(rewards.dtype) * REWARD_FINAL\n",
    "        rewards *= active_mask.float()\n",
    "\n",
    "        # track rewards and log probs for loss calculation at the end\n",
    "        all_rewards.append(rewards)\n",
    "        all_log_probs.append(log_probs * active_mask.float())\n",
    "        all_active.append(active_mask.float())\n",
    "        mean_entropies.append((entropies * active_mask.float()).sum() / active_mask.sum().clamp_min(1))\n",
    "\n",
    "    # prepare NxT matrices\n",
    "    LP = torch.stack(all_log_probs, dim=1)\n",
    "    RW = torch.stack(all_rewards, dim=1)\n",
    "\n",
    "    # calculate subsequent rewards\n",
    "    T = RW.shape[1]\n",
    "    d = (torch.arange(T, device=device)[:, None] - torch.arange(T, device=device)[None, :])\n",
    "    L = torch.where(d >= 0, (REWARD_DISCOUNT ** d).to(RW.dtype), torch.zeros_like(d, dtype=RW.dtype))\n",
    "    # L = torch.tril(torch.ones((T, T), device=device, dtype=RW.dtype))\n",
    "    G = RW @ L # NxT @ TxT -> NxT\n",
    "\n",
    "    # substract baseline\n",
    "    G_mean = G.mean(dim=0)\n",
    "    G -= G_mean\n",
    "\n",
    "    # calculate loss\n",
    "    M = torch.stack(all_active, dim=1)\n",
    "    denom = M.sum().clamp_min(1.0)\n",
    "    loss = -(LP * G).sum() / denom\n",
    "\n",
    "    avg_moves = boards.move_count.float().mean().item()\n",
    "    avg_entropy = sum(mean_entropies) / len(mean_entropies)\n",
    "    epoch_progress.set_description(f\"[epoch: {epoch + 1}] [loss: {loss.item()}] [avg moves: {avg_moves}] [avg entropy {avg_entropy}]\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleships-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
